# Shakespeare-Text-Generation-using-RNNs-LSTMs-and-GRUs

ğŸ“Œ Project Overview

This project explores the use of advanced Recurrent Neural Networks (RNNs) to perform creative text generation in the style of Shakespeare. Using the original plays as training data, the model learns to generate new, stylistically consistent text sequences.

We compare different RNN variants â€” Vanilla RNN, LSTM, and GRU â€” and evaluate them using quantitative (perplexity) and qualitative (text fluency) measures. To boost performance and realism, we also implement:

	â€¢	Temperature-controlled sampling
	â€¢	Beam search
	â€¢	Teacher forcing
	â€¢	Gradient clipping

â¸»

ğŸ¯ Objectives

	â€¢	Train and compare Vanilla RNN, LSTM, and GRU for text generation
	â€¢	Improve training stability and generation quality using advanced sequence modeling techniques
	â€¢	Evaluate text generation using perplexity, coherence, and fluency
	â€¢	Provide a reproducible pipeline for sequence generation with customizable hyperparameters

â¸»

ğŸ’¡ Value Proposition

	â€¢	Demonstrates deep learning proficiency in sequence modeling
	â€¢	Showcases creativity in model evaluation via both quantitative and qualitative analysis
	â€¢	Applies state-of-the-art generation techniques relevant to real-world NLP applications: chatbots, creative writing tools, summarizers, etc.
	â€¢	Offers employers a hands-on look at PyTorch NLP fluency, model tuning, and sequence generation expertise

â¸»

ğŸ§ª Techniques Used

ğŸ“š Dataset

	â€¢	Source: Shakespeare Corpus (shakespeare_plays.csv)
	â€¢	Filtered to use 3 representative plays: Hamlet, Coriolanus, and Richard III
	â€¢	Tokenized using Byte-Pair Encoding (BPE) with ByteLevelBPETokenizer

ğŸ§¼ Data Preprocessing

	â€¢	Character-level and subword tokenization
	â€¢	Sequence windowing with input-target pairs
	â€¢	Train-validation split and batching using torch.utils.data.DataLoader

ğŸ§  Model Architectures

	â€¢	Vanilla RNN: Simple RNN layers with hidden state propagation
	â€¢	LSTM (Long Short-Term Memory): Adds memory cell gates to improve long-term retention
	â€¢	GRU (Gated Recurrent Unit): A more efficient alternative to LSTM
	â€¢	All architectures use embedding layers and are implemented using PyTorch

âš™ï¸ Training Optimization

	â€¢	Teacher Forcing: To accelerate convergence and stability
	â€¢	Gradient Clipping (clip_grad_norm_): Prevents exploding gradients
	â€¢	Optimizer: Adam
	â€¢	Loss: Cross-entropy

ğŸ” Text Generation Strategies

	â€¢	Temperature-controlled sampling: Adjusts randomness in word selection
	â€¢	Beam Search: Produces more coherent outputs by keeping top-k sequences at each time step
	â€¢	Generation starts with seed input and stops at a pre-set length or punctuation

ğŸ“Š Evaluation

	â€¢	Perplexity: Measures model uncertainty; lower = better
	â€¢	Qualitative: Side-by-side comparison of text generated by different models
	â€¢	Samples include both default and beam search generations for each RNN type

â¸»
Future Enhancements

	â€¢	Integrate Transformer-based models (e.g., GPT-2, BART) for next-gen performance
	â€¢	Enable live deployment as a Shakespearean chatbot via Streamlit or Flask
	â€¢	Expand to multi-author stylized generation (e.g., compare Shakespeare vs. modern poetry)****
